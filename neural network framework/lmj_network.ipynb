{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Tuple, Dict\n",
    "import numpy\n",
    "\n",
    "NDArray = numpy.ndarray\n",
    "TENSOR_COUNTER = 0\n",
    "LAZY_MODE = False\n",
    "\n",
    "class ABC:\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def compute(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def gradient(self, *args: Any, **kwargs: Any) -> Any:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Op(ABC):\n",
    "    \"\"\"Operator definition.\"\"\"\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute(self, *args: Tuple[\"NDArray\"])->NDArray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def gradient_as_tuple(self, out_grad, node):\n",
    "        \"\"\" Convenience method to always return a tuple from gradient call\"\"\"\n",
    "        output = self.gradient(out_grad, node)\n",
    "        if isinstance(output, tuple):\n",
    "            return output\n",
    "        elif isinstance(output, list):\n",
    "            return tuple(output)\n",
    "        else:\n",
    "            return (output,)\n",
    "\n",
    "class Value:\n",
    "    op: Optional[Op]\n",
    "    inputs: List[\"Value\"]\n",
    "    cached_data: NDArray\n",
    "    requires_grad: bool\n",
    "\n",
    "    def realize_cached_data(self):\n",
    "        if self.cached_data is not None:\n",
    "            return self.cached_data\n",
    "        self.cached_data = self.op.compute(*[x.realize_cached_data() for x in self.inputs])\n",
    "        return self.cached_data\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.op is None\n",
    "\n",
    "    def _init(self, op: Optional[Op], inputs: List[\"Value\"],  *, num_outputs: int=1, cached_data: NDArray = None,\n",
    "        requires_grad: Optional[bool] = None\n",
    "    ):\n",
    "        if requires_grad is None:\n",
    "            requires_grad = any(x.requires_grad for x in inputs)\n",
    "        self.op = op\n",
    "        self.inputs = inputs\n",
    "        self.cached_data = cached_data\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @classmethod\n",
    "    def make_const(cls, data, *, requires_grad=False):\n",
    "        value = cls.__new__(cls)\n",
    "        value._init(\n",
    "            None,\n",
    "            [],\n",
    "            cached_data=data,\n",
    "            requires_grad=requires_grad,\n",
    "        )\n",
    "        return value\n",
    "    \n",
    "    def make_from_op(cls, op: Op, inputs: List[\"Value\"]):\n",
    "        value = cls.__new__(cls)\n",
    "        value._init(op, inputs)\n",
    "\n",
    "        if not LAZY_MODE:\n",
    "            if not value.requires_grad:\n",
    "                return value.detach()\n",
    "            value.get_outputs()\n",
    "        return value\n",
    "\n",
    "class Tensor(Value):\n",
    "    grad: \"Tensor\"\n",
    "    def __init__(self, array, *, dtype=\"float32\", requires_grad=True, **kwargs):\n",
    "        self.array = numpy.array(array, dtype=dtype)\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_numpy(numpy_array, dtype):\n",
    "        return numpy.array(numpy_array, dtype=dtype)\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_from_op(op: Op, inputs: List[\"Value\"]):\n",
    "        tensor = Tensor.__new__(Tensor)\n",
    "        tensor._init(op, inputs)\n",
    "        if not LAZY_MODE:\n",
    "            tensor.realize_cached_data()\n",
    "        return tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_const(data, requires_grad=False):\n",
    "        tensor = Tensor.__new__(Tensor)\n",
    "        if isinstance(data, Tensor):\n",
    "            tensor_data = data.realize_cached_data()\n",
    "        else:\n",
    "            tensor_data = data\n",
    "        tensor._init(None, cached_data=tensor_data, requires_grad=requires_grad)\n",
    "        return tensor\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.cached_data\n",
    "    \n",
    "    def detach(self):\n",
    "        return Tensor.make_const(self.realize_cached_data())\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.cached_data = value.realize_cached_data()\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.realize_cached_data().shape\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.realize_cached_data().dtype\n",
    "\n",
    "    def backward(self, out_grad=None):\n",
    "        if out_grad:\n",
    "            out_grad = out_grad\n",
    "        else:\n",
    "            out_grad = Tensor(numpy.ones(self.shape))\n",
    "        compute_gradient_of_variables(self, out_grad)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return EWiseAdd()(self, other)\n",
    "        else:\n",
    "            return AddScalar(other)(self)\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return EWiseMul()(self, other)\n",
    "        else:\n",
    "            return MulScalar(other)(self)\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return EWiseAdd()(self, Negate()(other))\n",
    "        else:\n",
    "            return AddScalar(-other)(self)\n",
    "\n",
    "\n",
    "def compute_gradient_of_variables(output_tensor, out_grad):\n",
    "    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {} # dict结构，用于存储partial adjoint\n",
    "    node_to_output_grads_list[output_tensor] = [out_grad] \n",
    "    reverse_topo_order = list(reversed(find_topo_sort([output_tensor]))) # 请自行实现拓扑排序函数\n",
    "    for node in reverse_topo_order:\n",
    "        node.grad = sum(node_to_output_grads_list[node]) # 求node的partial adjoint之和，存入属性grad\n",
    "        if node.is_leaf():\n",
    "            continue\n",
    "        for i, grad in enumerate(node.op.gradient(node.grad, node)): # 计算node.inputs的partial adjoint\n",
    "            input_node = node.inputs[i]\n",
    "            if input_node not in node_to_output_grads_list:\n",
    "                node_to_output_grads_list[input_node] = []\n",
    "            node_to_output_grads_list[input_node].append(grad)\n",
    "\n",
    "def find_topo_sort(node_list: List[Value]) -> List[Value]:\n",
    "    visited = set()\n",
    "    topo_order = []\n",
    "    topo_sort_dfs(node_list[-1], visited, topo_order)\n",
    "    return topo_order\n",
    "\n",
    "\n",
    "def topo_sort_dfs(node, visited, topo_order):\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    for pre_node in node.inputs:\n",
    "        topo_sort_dfs(pre_node, visited, topo_order)\n",
    "    topo_order.append(node)\n",
    "\n",
    "class TensorOp(Op):\n",
    "    def __call__(self, *args):\n",
    "        return Tensor.make_from_op(self, args)\n",
    "\n",
    "class EWiseAdd(TensorOp):\n",
    "    def compute(self, a: NDArray, b: NDArray):\n",
    "        return a+b\n",
    "    \n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return out_grad, out_grad\n",
    "    \n",
    "class AddScalar(TensorOp):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def compute(self, a: NDArray):\n",
    "        return a + self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return (out_grad, )\n",
    "    \n",
    "class EWiseMul(TensorOp):\n",
    "    def compute(self, a: NDArray, b: NDArray):\n",
    "        return a * b\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        a, b = node.inputs\n",
    "        return out_grad * a, out_grad * b\n",
    "    \n",
    "class MulScalar(TensorOp):\n",
    "    def __init__(self, scalar):\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def compute(self, a: NDArray):\n",
    "        return a * self.scalar\n",
    "\n",
    "    def gradient(self, out_grad: Tensor, node: Tensor):\n",
    "        return (out_grad * self.scalar,)\n",
    "    \n",
    "class Negate(TensorOp):\n",
    "    def compute(self, a):\n",
    "        return numpy.negative(a)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        return MulScalar(-1)(out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class ABC():\n",
    "    pass\n",
    "\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def reset_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr = 0.001):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.params):\n",
    "            grad = Tensor(param.grad, dtype='float32').data\n",
    "            param.data = param.data - grad * self.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(TensorOp):\n",
    "    def compute(self, a):\n",
    "        return numpy.maximum(a, 0)\n",
    "\n",
    "    def gradient(self, out_grad, node):\n",
    "        a = node.inputs[0].realize_cached_data()\n",
    "        return out_grad * Tensor(a > 0)\n",
    "\n",
    "\n",
    "def relu(a):\n",
    "    return ReLU()(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def randn(*shape, mean=0.0, std=1.0, dtype=\"float32\", requires_grad=False):\n",
    "    array = np.random.randn(*shape) * std + mean\n",
    "    return array\n",
    "\n",
    "def init_He(in_features, out_features):\n",
    "    s = np.random.normal(0, 1/in_features, in_features*out_features)\n",
    "    s = s.reshape(in_features, out_features)\n",
    "    return s\n",
    "\n",
    "def init_Xavier(in_features, out_features, dtype):\n",
    "    v = math.sqrt(2/(in_features+out_features))\n",
    "    return randn(in_features, out_features,std=v, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from functools import reduce\n",
    "from typing import List, Any\n",
    "\n",
    "class ABC:\n",
    "    pass\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def parameters(self)->List[\"Tensor\"]:\n",
    "        return _unpack_params(self.__dict__)\n",
    "    \n",
    "    def _children(self):\n",
    "    # 遍历当前对象属性，找出子模块（例如是 Module 类型的实例）\n",
    "        children = []\n",
    "        for attr in self.__dict__.values():\n",
    "            if isinstance(attr, Module):\n",
    "                children.append(attr)\n",
    "        return children\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "        for m in self._children():\n",
    "            m.training = False\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        for m in self._children():\n",
    "            m.training = True\n",
    "\n",
    "\n",
    "class Parameter(Tensor):\n",
    "    pass\n",
    "\n",
    "def _unpack_params(value: object) -> List[Tensor]:\n",
    "    if isinstance(value, Parameter):\n",
    "        return [value]\n",
    "    elif isinstance(value, Module):\n",
    "        return value.parameters()\n",
    "    elif isinstance(value, dict):\n",
    "        params = []\n",
    "        for k, v in value.items():\n",
    "            params += _unpack_params(v)\n",
    "        return params\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        params = []\n",
    "        for v in value:\n",
    "            params += _unpack_params(v)\n",
    "        return params\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def _child_modules(value: object) -> List[\"Module\"]:\n",
    "    if isinstance(value, Module):\n",
    "        modules = [value]\n",
    "        modules.extend(_child_modules(value.__dict__))\n",
    "        return modules\n",
    "    if isinstance(value, dict):\n",
    "        modules = []\n",
    "        for k, v in value.items():\n",
    "            modules += _child_modules(v)\n",
    "        return modules\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        modules = []\n",
    "        for v in value:\n",
    "            modules += _child_modules(v)\n",
    "        return modules\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=\"float32\"):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        s = init_He(in_features, out_features)\n",
    "        self.weight = Parameter(s, dtype=\"float32\")\n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(self.out_features),dtype=\"float32\")\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        X_out = X @ self.weight\n",
    "        if self.bias:\n",
    "            return X_out + self.bias.broadcast_to(X_out.shape)\n",
    "        return X_out\n",
    "    \n",
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, x: Tensor)->Tensor:\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x\n",
    "    \n",
    "class relu(Module):\n",
    "    def forward(self, x: Tensor)->Tensor:\n",
    "        return relu(x)\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, X):\n",
    "        size = reduce(lambda a, b: a * b, X.shape)\n",
    "        return X.reshape((X.shape[0], size // X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, X, y, transforms: Optional[List] = None):\n",
    "        self.transforms = transforms        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index) -> object:\n",
    "        imgs = self.X[index]\n",
    "        labels = self.y[index]\n",
    "        if len(imgs.shape) > 1:\n",
    "            imgs = np.vstack([self.apply_transforms(img.reshape(28, 28, 1)) for img in imgs])\n",
    "        else:\n",
    "            imgs = self.apply_transforms(imgs.reshape(28, 28, 1))\n",
    "        return (imgs, labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def apply_transforms(self, x):\n",
    "        if self.transforms is not None:\n",
    "            for tform in self.transforms:\n",
    "                x = tform(x)\n",
    "        return x\n",
    "\n",
    "class DataLoader:\n",
    "    dataset: Dataset\n",
    "    batch_size: Optional[int]\n",
    "\n",
    "    def __init__(self, dataset: Dataset, batch_size: Optional[int] = 1, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        if not self.shuffle:\n",
    "            self.ordering = np.array_split(np.arange(len(dataset)), range(batch_size, len(dataset), batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.ordering = np.array_split(np.random.permutation(len(self.dataset)), range(self.batch_size, len(self.dataset), self.batch_size))\n",
    "        self.idx = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.idx += 1\n",
    "        if self.idx >= len(self.ordering):\n",
    "            raise StopIteration\n",
    "        batch_indices = self.ordering[self.idx]\n",
    "        return tuple([Tensor(x, device = self.device) for x in self.dataset[batch_indices]])\n",
    "'''\n",
    "def load_svmfile(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, 'r') as f:\n",
    "        filelines = f.readlines()\n",
    "        for fileline in filelines:\n",
    "            fileline = fileline.strip().split(' ')\n",
    "            #print(fileline)\n",
    "            Y.append(int(fileline[0]))\n",
    "            tmp = []\n",
    "            for t in fileline[1:]:\n",
    "                if len(t)==0:\n",
    "                    continue\n",
    "                tmp.append(float(t.split(':')[1]))\n",
    "            X.append(tmp)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "dataset1 = 'svmguide1.bin'\n",
    "dataset2 = 'svmguide1.t.bin'\n",
    "print('Start loading dataset {}'.format(dataset1))\n",
    "X, Y = load_svmfile(dataset1) # train set\n",
    "X_test, Y_test = load_svmfile('{}'.format(dataset2))\n",
    "training_data = Dataset(X, Y)\n",
    "test_data = Dataset(X_test, Y_test)\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear_relu_stack = Sequential(\n",
    "            Linear(28*28, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 512),\n",
    "            ReLU(),\n",
    "            Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # 将model设为训练模式，对batch normalization和dropout很重要\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 计算模型输出和损失\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # 将model设为测试模式，对batch normalization和dropout很重要\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "def CrossEntropyLoss(y, pred):\n",
    "    c = 0\n",
    "    for i in range(len(y)):\n",
    "        a = 0\n",
    "        for j in range(len(y[i])):\n",
    "            a += -y[i][j] * math.log(pred[i][j])\n",
    "        c += a\n",
    "    return c\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fn = CrossEntropyLoss\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
